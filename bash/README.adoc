= Yseop Bash guidelines and tips
:toc:
:toclevels: 3
:imagesdir: readme-img

:do_not:  &#x1F44E; Do not write this
:instead: &#x1F44D; Write this instead


== Style

Things that do not change what’s executed but may derive profit from being harmonized within the company.


=== Indentation

To align with other languages used at Yseop, favor:

* [x] Spaces, not hard tabs.
* [x] Width: 4 (Non-Ops projects) or 2 (Ops projects).
Try to be *consistent* within a Git repository, and even more so within a single script.


=== Naming

Variables and functions:: `snake_case`

Global `readonly` variables that do not depend on the script’s input and are set early (basically; your mileage may vary)::
`FULL_CAPS_SNAKE_CASE` (aka `SCREAMING_SNAKE_CASE`)
+
[CAUTION]
====
Global readonly variables behave in a specific way and tend to prevent the use of their names for local variables in functions:

[source, bash]
----
  $ readonly x=1; f() { local x=2; }; f
bash: local: x: readonly variable
----

This all-caps rule prevents such collisions and allows to name our local variables without needless worries, in addition to https://en.wikipedia.org/wiki/Constant_(computer_programming)#Naming_conventions[aligning] with what is done in https://en.wikipedia.org/wiki/Naming_convention_(programming)#Language-specific_conventions[countless programming languages] and https://google.github.io/styleguide/shellguide.html#s7.3-constants-and-environment-variable-names[renowned style guides].
====

Script files:: `kebab-case.sh`

.All in all
====
.`write-plop.sh`
[source, bash]
----
write_text_to_plop() {
    printf '%s\n' "$1" > "$PATH_TO_PLOP"
}

readonly PATH_TO_PLOP=foo/bar/plop.txt

the_text=$1
write_text_to_plop "$the_text"
----
====


=== Typical elements

==== Comments

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
foo # Bar
----
|
[source, bash]
----
# Bar
foo
----
|===

Also note that `+#Bar+` is generally used for commented-out code (typically during a debugging session), while explanatory comments should use a leading space: `+# Bar+`.

See also <<documentation>>.


[[documentation]]
==== Documentation

There is no set Javadoc-like convention in shell scripts, but this does not mean that we should not add documentation.
Try to add something that looks like the example below…

* at the top of each script (keep it short if there is a `--help` feature in it, to avoid duplication);
* above each non-trivial function.

====
[source, bash]
----
# Print a list of fruits available on the server.
#
# The “SERVER_USR” and “SERVER_PSW” environment variables must be set. <1>
#
# The global associative array “censored_fruits” must be initialized. <2>
# The fruits contained in it will be removed from the printed list.
#
# $1    Base API URL of the server. <3>
# $2    (Optional) If set and not empty, use HTTPS instead of HTTP. <4>
#       Note that a valid certificate is needed.
#
# stdout →  JSON array, each item being an object having the following string <5>
#           fields: “name”, “id” <6>
#           If there are no fruits, an empty array (“[]”) is printed.
#
# Returns with a 0 status if and only if the query was sent successfully. <7>
----
====
<1> Oftentimes, shell scripts need things that are not passed very explicitly, so listing them could make people’s life easier.

<2> For global variables (which are sometimes hard to avoid, even if it’s indeed better to avoid them), it can be useful to state whether they are strings (the most common ones), integers (rare, declared with `declare -i` or `local -i`), indexed arrays or associative arrays (the hashmap-like ones).
At least you then know what you’re dealing with when performing maintenance on the function’s content.

<3> Use the Tabulation key to neatly align (with spaces, as usual) parameter descriptions.

<4> Argument number and usage is often far from evident in shell scripts and functions, so make sure you list them all and mark optional arguments as such.

<5> Results are generally printed out.
Do not mix that up with the return status provided via `return` in functions and `exit` in scripts, which is merely used to indicate… well… a status.
Therefore, never say that a function or script “returns” text.
It can put text in a global variable, print it to stdout or stderr, or even write it into a file, but it never “returns” it.

<6> Wrap when lines get arguably long.

<7> Do not use “returns” and “exits” interchangeably: a function may use `exit` to completely abort the script that called it – it is very different from using `return`.
It follows that any given function may have, simultaneously, explanatory “Returns with…” and “Exits with…” sentences in its documentation.


==== Shebang

The battle between `/bin/bash` and `/usr/bin/env bash` has been setting the web ablaze for years.
None seems inherently better.
Let’s not care too much about it.
If one day one of them causes issues in one particular context, we’ll switch to the other there and that’ll be the end of the story.
Try to keep at least one blank line after the shebang, though.

.Invoke a precise executable; allows to pass options like `-x` to `bash` directly there
[source, bash]
----
#! /bin/bash

# […]
----

.Use `$PATH` to look for the `bash` program
[source, bash]
----
#! /usr/bin/env bash

# […]
----


==== Functions

* Favor the more-standardized `foo()` over `function foo` or `function foo()`.

* Preferably with the opening brace after the name, but not extra important.
As for indentation width, check for consistency, though.

[source, bash]
----
foo() {
    local bar
    bar=$(plop yo)
    palala "$bar"
}
----


==== `if`

[source, bash]
----
if foo
then
    plop
else
    yo
fi
----


==== `for`, `while`, `until`, `select`

[source, bash]
----
for a in plop yo
do
    stuff "$a"
done
# Idem for “select”.
----

[source, bash]
----
while [[ $a -gt 1 ]]
do
    ((a /= 2))
done
# Idem for “until”.
----

[source, bash]
----
while ((a /= 2))
    echo 'Hi there, we’re doing a do-while here instead of while-do.'
    [[ $a -gt 1 ]]
do
    # No-op.
    :
done
# Idem for “until”.
----


==== `case`

[source, bash]
----
case "$a" in
    a) short;;

    *)
        long
        long
        long
        ;;
esac
----


==== Strings

===== Quote types

In shell scripts, nearly everything is a string.
Quotes are not really there to _define_ strings: they are a tool to _escape characters_ – especially whitespace.
Kinda like a massive backslash targeting multiple characters at once.

Both single (`'`) and double quotes (`"`) have their use in scripts.
You can find https://stackoverflow.com/questions/6697753/difference-between-single-and-double-quotes-in-bash[lots of details about that] on the web, but, _very_ basically:

* In `"foo $(bar) ${plop} $((yo + 1)) yeah"`, the `$`-based things are interpreted: command executions, variable substitution, arithmetics…

* In `'foo $(bar) ${plop} $((yo + 1)) yeah'`, nothing happens: you get this string, verbatim.

Double quotes can roughly be seen like https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals[backticks in JavaScript, where they are used for template strings].
*Don’t use double quotes when you don’t need any `$`-based construct in your string!*
Stick to single quotes if you are just giving a *hardcoded, fixed string*.
Depending on the context, you may even be better off with *no quotes at all*: single quotes are often more a visual hint than an obligation.

What matters is that you don’t send readers on the wrong tracks regarding *your intent* and the *potential content, meaning, and role* of the string.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
foo=$("grep" "plop" "path/to/file.txt")
if [ "$foo" = "yo-plop" ]
then
    "printf" "Result: %s\n" "Success"
fi
----
|
[source, bash]
----
foo=$(grep 'plop' path/to/file.txt)
if [ "$foo" = 'yo-plop' ]
then
    printf 'Result: %s\n' 'Success'
fi
----
|===

[TIP]
====
The special syntax `$'…'` allows the interpretation of “ANSI{nbsp}C” backslash-escaped characters – most famously `\n` for newlines and `\t` for tabulations, but also Unicode-related sequences:

[source, bash]
----
$ nl <<< $'foo\tbar\nplop \u2665'
     1	foo<TAB>bar
     2	plop ♥
----
====

See also <<unprotected_expansions>> and <<quotes_in_assignments>>.


===== Concatenation

Just put strings side by side, and they’ll be merged, since quotes are removed once they have fulfilled their purpose of escaping what’s within.

For more complex or programmatic concatenations, use `+=`.

====
[source, bash]
----
foo=bar$(echo yo)'s p a c e s'$((1 + 2))$'tab\ttab'
bar=123
foo+="s p a c e s${bar} again"
----

⇒ `foo` will contain `baryos p a c e s3tab<TAB>tabs p a c e s123 again`.
====


[[heredoc]]
==== Here documents

(The `<<` technique used to feed multiple lines into a command, typically for help blurbs; see `man bash`.)

* [x] The tag used to denote the end of the document should follow the `+_[A-Z][A-Z0-9_]*_+` format: `+_HELP_+`, `+_JSON_+`, `+_MESSAGE_+`…

* [x] The tag should be meaningful and suit the document’s purpose:
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
cat << eof
<?xml version="1.0" encoding="UTF-8"?>
<note>
  <to>You</to>
</note>
eof
----
|
[source, bash]
----
cat << _XML_
<?xml version="1.0" encoding="UTF-8"?>
<note>
  <to>You</to>
</note>
_XML_
----
|===

* [x] If there is no particular reason to allow any kind of Bash expansion within the document, quote the opening tag to prevent them altogether, thus making your intentions clear and prevent unwanted dollar disappearances or whatever:
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
cat << _TEXT_
This should appear as-is: $foo
_TEXT_
----
|
[source, bash]
----
cat << '_TEXT_'
This should appear as-is: $foo
_TEXT_
----
|===

* [x] Avoid the `+<<-+` syntax (used to remove leading hard tabs) unless you really need it.
Keep in mind that it does not remove leading spaces, only hard tabs (which we do not use anyway).


==== Arrays

Keep your array declarations (or item additions) readable with linebreaks and indentation, unless they are really trivial.
This will also make Git conflicts less likely.

[source, bash]
----
t1=(
    foo
    bar
    'an item with whitespace'
    "$(some_command 'arg')"
)

t2+=(
    yo
    yeah
)

declare -A associative_t=(
    [key1]='value 1'
    [key2]='value 2'
)

----

// ↑ ↑ ↑ Oddly enough, Visual Studio Code’s syntax highlighting goes haywire
// if I remove the blank line between the “)” and the end of the snippet.


==== Redirections

Use one space on each side of the redirection operator:

* [x] Between the command and the operator.
* [x] Between the operator and the file path or here string or <<heredoc, here document>>.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
f>log.txt
f>>log.txt
f 2>log.txt
f&>/dev/null
f 2>&1

f<input.txt
f<<<here_string
----
|
[source, bash]
----
f > log.txt
f >> log.txt
f 2> log.txt
f &> /dev/null
f 2>&1

f < input.txt
f <<< here_string
----
|===

[CAUTION]
====
Things like `2>&1` are actually one big operator.
They’ll break if you split them.
====


== Pitfalls

Things that may cause issues in the long run or that may hamper initial development.


[[unprotected_expansions]]
=== Unprotected expansions

When writing `$foo` on its own instead of `"$foo"`, you’re asking the Bash interpreter to split the contents of the `foo` variable into multiple words in every place where a character of the `IFS` variable (“Internal Field Separator” – by default this includes spaces, tabs and newlines) is found:

[source, bash]
----
  $ count() { printf 'I got %d arguments.\n' "$#"; }

  $ txt='foo bar'

  $ count $txt
I got 2 arguments.

  $ count "$txt"
I got 1 arguments.
----

This is basically only useful when you’re implementing a “split” function yourself, and even then it is strongly advised to set `IFS` explicitly and locally (using parentheses):

[source, bash]
----
split() {
    (
        IFS=$1
        printf '%s\n' $2
    )
}
----

.`split a falalap`
----
f
l
l
p
----

.`split l falalap`
----
fa
a
ap
----

… and even then, most of the time `xargs` can be used to avoid both the unprotected expansion _and_ the `IFS` tempering:

[source, bash]
----
split() {
    printf '%s' "$2" | xargs --delimiter "$1" printf '%s\n'
}
----

In other cases, if you _need_ a variable to be expanded into _multiple_ words to provide arguments to a command, use an indexed array, as this is one of the things they were made for.
This takes away all concerns regarding potential unwanted word-splitting and `IFS` itself.
This is why not quoting a variable “on purpose” is sometimes described as an “anti-pattern”: it can often be harmful, and has no benefit whatsoever with respect to using arrays.

[quote, Google, https://google.github.io/styleguide/shellguide.html#quoting]
____
* Always quote strings containing variables, command substitutions, spaces or shell meta characters, unless careful unquoted expansion is required or it’s a shell-internal integer.
(*Yseop note:* Even internal integers can get split if `IFS` contains digits, so beware.)

* Use arrays for safe quoting of lists of elements, especially command-line flags.

* Use `"$@"` unless you have a specific reason to use `$*`, such as simply appending the arguments to a string in a message or log.
____

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
opts='-p -v'
mkdir $opts path/to/plop.log
----
|
[source, bash]
----
opts=(-p -v)
mkdir "${opts[@]}" path/to/plop.log
----
|===


[[wd]]
=== Working directory

Avoid using `cd` (or changing the working directory in any other way) if possible.
Relative paths that may have been provided as arguments would become invalid, for example.

It follows that you should use absolute paths internally whenever you can.
See <<basedir>>, as this can help greatly in that regard.

[TIP]
====
You can use `cd` in a subshell (`$(…)` or `(…)`) so that effects will be reverted.
Also see `pushd` and `popd`.
====


=== `eval`

`eval` has several major issues:

* It is known to often open the door for code injection, which is a significant security threat.

* Especially when variables or whitespace are involved, it forces developers to “think forward” and nest quotation marks while trying to figure out _which_ step will need _which_ quotes to perform _which_ operation.
Furthermore, the solution in such cases often depends on the specific matter at hand.

Before using `eval`, make extra sure you cannot:

* [x] Directly run your command without storing it in a variable beforehand (you can put it in a function if it is used multiple times):
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
cmd='grep -r foo bar/'
eval "$cmd"
----
|
[source, bash]
----
grep -r foo bar/
----
|===
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
awk_script='/foo/ { print $2 }'
cmd='awk "$awk_script"'

eval "$cmd" file1
eval "$cmd" file2
eval "$cmd" file3
----
|
[source, bash]
----
call_awk() {
    local awk_script='/foo/ { print $2 }'
    awk "$awk_script"
}

call_awk file1
call_awk file2
call_awk file3
----
|===

* [x] Store your command (or arguments, or options) in an indexed array instead, especially if that command must be built dynamically.
See https://stackoverflow.com/a/71600549/9089759[this StackOverflow answer] and https://google.github.io/styleguide/shellguide.html#arrays[Google’s guide].
+
> Using a single string for multiple command arguments should be avoided, as it inevitably leads to authors using `eval` or trying to nest quotes inside the string, which does not give reliable or readable results and leads to needless complexity.

* [x] For cases where you need to handle a variable whose name must be deduced from another variable: use `declare -n foo=$bar`, `local -n foo=$bar`, or `${!bar}`.
This is also covered in https://stackoverflow.com/a/71600549/9089759[that same StackOverflow answer].
See also <<pass_by_name, this section>> regarding `declare -n` and `local -n`.


=== `set -e` and others

(See `help set` for details.)


==== `-e`: “Exit immediately if a command exits with a non-zero status.”

Putting `set -e` at the beginning of a script is often used to prevent “snowballing”: most errors will cause the script to immediately halt.

This is not _always_ relevant, though:

* https://stackoverflow.com/questions/71619652/difference-of-behavior-between-set-e-source-and-bash-ec-source[It is not as easy to master as it seems.]
There are many contexts in which commands are legitimately allowed by `set -e` to fail (typically in conditionals, or on the left side of a pipe), and not knowing them could lead to more trouble than a fully manual error handling strategy would.

* In a script meant for internal use on non-sensitive data, it may not be worth the shift of mindset it requires.

Furthermore, there are a few things (edit: a _lot_ of things) to know to avoid mind-boggling issues:

When doing `+((n--))+`, for example, to decrement a variable, the `+((…))+` construct has an exit status of its own.
This status is a success status _if and only if_ the result of the computation is different than{nbsp}0:

[source, bash]
----
  $ ((0)); echo "$?"
1

  $ ((1)); echo "$?"
0

  $ ((-1)); echo "$?"
0
----

This means that reaching zero while decrementing your variable would cause the script to end if you activated `set -e`!
To prevent this and be on the safe side, you can do:

[source, bash]
----
  $ ((0)) || true; echo "$?"
0
----

Basically, `|| true` is an easy way to tell `set -e` that it does not matter if the command right before it fails (`true` will be executed, succeed, and the status of the command chain it formed will be a success status).

Regarding `set -e`’s shortcomings and oddities, you can also read:

* https://web.archive.org/web/20220314040936/http://mywiki.wooledge.org/BashFAQ/105

* https://stackoverflow.com/questions/71619652/difference-of-behavior-between-set-e-source-and-bash-ec-source


==== Keeping strange settings local

Every `+set -<something>+` (or `set {plus}++<something>++`, for that matter) using flags other than `-e -x -v` should ideally stand within a subshell to make sure you do not wreak havoc by forcing a significant paradigm shift onto your workmates (or yourself) in a huge script.

Most of the default settings were chosen with good reasons, and sometimes, while it may seem that doing, for example, `set -u` (“Treat unset variables as an error when substituting.”) is a good idea, you may just be breaking ten things while fixing the _one specific_ instruction you’re working on.

[NOTE]
====
There is a certain degree of tolerance regarding `set -e`, as it has been widely spread in the company, but as explained in the previous section, it is tricky to use, so keep it local (or avoid using it altogether) if you can.
====

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash, subs = "+attributes"]
----
set -evxu -o pipefail -o noclobber
# […] lots of stuff
foo {vbar} bar "$mandatory" > out.log
----
|
[source, bash, subs = "+attributes"]
----
set -evx
# […] lots of stuff
(
    set -u -o pipefail -o noclobber
    foo {vbar} bar "$mandatory" > out.log
)
----
|===

[NOTE]
====
The same goes for changes brought to sensitive variables – typically `IFS`, which is used for word-splitting and to join array items when using `+[*]+`.

[source, bash]
----
t=(foo bar)
(
    IFS=','
    printf 'Joined with commas: %s\n' "${t[*]}"
)
----

⇒ `Joined with commas: foo,bar`
====


=== Backticks

The `foo={backtick}echo plop{backtick}` syntax is deprecated and can lead to all kinds of trouble when attempting to nest things.
It has no advantage whatsoever (beside the slightly lower character count) over `$(…)`.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash, subs = "+attributes"]
----
foo=`bar plop`
yo "`pa la la`" hey
----
|
[source, bash, subs = "+attributes"]
----
foo=$(bar plop)
yo "$(pa la la)" hey
----
|===


=== Loss of values when exiting a loop

Pipes spawn subshells to execute what’s on they right-hand side.
A common consequence of that, which can lead to much headache, is that changes brought to variables get reverted when exiting some loops:

.This will yield 0, not 1+2+3=6
====
[source, bash]
----
n=0
seq 3 | while read k
do
    ((n += k))
done
printf '%d\n' "$n"
----
====

Solutions include:

. Using “process substitution”:
+
[source, bash]
----
n=0
while read k
do
    ((n += k))
done < <(
    seq 3
)
printf '%d\n' "$n"
----

. Using a “here string”:
+
[source, bash]
----
data=$(seq 3)
n=0
while read k
do
    ((n += k))
done <<< "$data"
printf '%d\n' "$n"
----

[TIP]
====
See `man bash` for more on those concepts.
====


=== Make local variables actually local

By default, when you write `foo=bar`, you create a _global_ `foo` variable.
While this may at first seem convenient, this can lead to strange and annoying bugs or false positives, like a function working correctly (only for a while…) despite not actually using the arguments passed to it, picking global variables instead without you being conscious of it.

A good habit is to mark as `local` every variable meant to be used solely within the function that declares them:

[source, bash]
----
my_sub_function() {
    printf 'In sub-function: %s\n' "$foo"
}

my_function() {
    local foo=notplop
    printf 'In function: %s\n' "$foo"

    my_sub_function
}

foo=plop
my_function
printf 'After function call: %s\n' "$foo"
----

This yields:

[source]
----
In function: notplop
In sub-function: notplop
After function call: plop
----

The variable bubbled down to `my_sub_function`, but did not interfere with the global `foo` variable used outside of those functions.

As a bonus, `local` guarantees that the variables are empty upon declaration.
No need for weird `unset -v`, `foo=''` or whatever.


=== `echo` vs. `printf`

The options and escape sequences interpreted by `echo` depends heavily on the considered platform, which `printf` is much more standardized.

To quote https://pubs.opengroup.org/onlinepubs/9699919799/utilities/echo.html#tag_20_37_16[the Open Group]:

> It is not possible to use `echo` portably across all POSIX systems unless both `-n` (as the first argument) and escape sequences are omitted.

At Yseop, we once had a release note that was heavily truncated because the system that ran the release note-generating script was interpreting, in `echo`, by default, the `+\c+` sequence, which means “suppress further output” and happened to appear in YML snippets as the beginning of commands like `+\command+`.

.OK
* [x] `echo 'foo bar' plop` (No option, hardcoded parameters.)

* [x] `echo` (No argument, just printing a newline.)

.Not OK
* [ ] `echo "$plop"`
(Dynamic content, safer to use `printf '%s\n' "$plop"`.)

* [ ] `echo "Plop: $plop"`
(Idem, and would benefit from a clear formatting string; safer to use `printf 'Plop: %s\n' "$plop"`.)


=== (Not) parsing `ls`’s output

`ls` is for humans and is far too unreliable.
Machines prefer `find`, `stat`, or globbing patterns like `foo/*`.
(This topic is heavily covered on the web.)


== Overkill

Things that can be done in more concise or clear ways.


[[quotes_in_assignments]]
=== Quotes in assignments

Quotes are not needed on the right-hand side of assignments, unless you have hardcoded spaces or Bash metacharacters (`| & ;`, etc.) there.

[source, bash]
----
print_stuff_with_spaces() {
    echo abc
    echo
    echo d e f
}

var_with_spaces=$'a \t b \n c'

mix=foo${var_with_spaces}bar$(
    print_stuff_with_spaces
)plop

printf 'mix = [%q]\n' "$mix"
----

This yields (as expected):

[source]
----
mix = [$'fooa \t b \n cbarabc\n\nd e fplop']
----

The mandatory uses for quotes in assignments are things like:

[source, bash]
----
foo='bar plop'
#       ↑
# Hardcoded spaces
#       ↓
foo="bar ${yo}"

foo='bar&plop'
#       ↑
# Bash metacharacters
#       ↓
foo="bar;${yo}"
----

Of course, when in doubt, it’s better to quote, especially if it leads to a better syntax highlighting, but try to *avoid overkill things*:

* `foo=$1` or `n=12` are perfectly readable without quotes.
* `foo=$(…)` looks less cluttered than `foo="$(…)"` while being just as robust.


=== Reading input

Many commands can read data by accepting files as parameters, or by getting data on their standard input (which can be made easier via redirections).
https://web.archive.org/web/20220327013356/https://porkmail.org/era/unix/award#cat[Keep in mind that `cat`’s name stands for “concatenate”], not for “dump this file’s content on stdout because I have a grudge against everything that is not a pipe”.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash, subs = "+attributes"]
----
cat "$f" {vbar} grep plop
cat "$f" {vbar} tr -cd 'a-z'
printf '%s\n' "$PASSWORD" {vbar} docker login -u me --password-stdin
----
|
[source, bash, subs = "+attributes"]
----
grep plop "$f"
tr -cd 'a-z' < "$f"
docker login -u me --password-stdin <<< "$PASSWORD"
----
|===


=== `sed`’s `-e`

This option is https://unix.stackexchange.com/a/387528[virtually _never_ useful].
It is even generally harmful as far as readability is concerned, since you can, instead of using it, write your `sed` script on multiple lines, possibly even with comments:

[source, bash]
----
sed '
    # Censor.
    /crap/ d

    # Yell.
    s/.*/\U&/g
' "$my_file"
----


=== `-n` in `[` and `[[`

Another generally useless option.

`[ -n "$foo" ]` +
⇔ +
`[ "$foo" ]` +
⇔ +
`[[ -n $foo ]]` +
⇔ +
`[[ $foo ]]`

[NOTE]
====
There’s an exception:
`[[` allows line breaks (which is awesome for readability) but may, then, get confused:

> unexpected token “newline”, conditional binary operator expected

In such cases, so you may need that overkill option:

[source, bash]
----
if [[
    -n $1
    &&
    -n $2
]]
----
====

See `help test` for `[` and `help [[` for `[[` when in doubt.


== Maintainability

Make your code easier to understand for others.


=== `&&` and `||`

Do not use those as “the lazy coder’s `if-then-else`”.
These are often misunderstood and misused.

* Real, human-friendly flow-control keywords make the structure of the code more apparent.

* A `foo && bar || plop` chain can actually run both `bar` and `plop` depending on what happens, so it’s _not_ a viable makeshift ternary operator.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash, subs = "+attributes"]
----
my_func && grep plop plup {vbar}{vbar} echo omg
----
|
[source, bash]
----
if my_func
then
    grep plop plup
else
    echo omg
fi
----
|===

Legit uses include:

. Situations where the right-hand side consists of nothing but a _very basic statement_ such as:
+
--
** `true`,
** `false`,
** `continue`,
** `break`,
** `return`,
** `exit`.
--
+
[source, bash]
----
for file in ./*.txt
do
    # Get rid of garbage in case the globbing pattern
    # matched nothing and came back as-is.
    [[ -r $file ]] || continue
    # […] process
done
----

. Conditionals:
+
[source, bash]
----
if [[ $foo && ! $bar ]] || [ -f "$path" ]
then
    echo plop
fi
----

Additionally, note that it is possible to start a new line after `&&` or `||` (you can view those markers as “special kinds of semicolons”, much like the single `&`).
There is often no good reason not to do so (outside of the “very simple statements” cases mentioned above):

[source, bash]
----
create_file foo/bar.txt &&
grep yo foo/bar.txt &&
echo 'OK!'

if long_condition_involving_a_function and its args &&
    another_thing_that_must_succeed and other args
then
    echo wow
fi
----


=== Single vs. double quotes

Double quotes allow expansions such as:

* `+$foo+`
* `+${foo//a/_}+`
* `+$(bar)+`
* `+$((1 + n))+`

Single quotes do not allow this.

Avoid double quotes when you do not need any kind of expansion.
Using double quotes in irrelevant places is akin to saying to readers: “Hey, look! I do fancy stuff in there! … `[Ten good seconds later]` Just kidding! There’s actually nothing special to see!”

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
txt="Hello there, this is a message."
printf "%s %s\n" "$txt" "foo bar"
----
|
[source, bash]
----
#   ↓                               ↓
txt='Hello there, this is a message.'
printf '%s %s\n' "$txt" 'foo bar'
#      ↑       ↑        ↑       ↑
----
|===

[TIP]
====
Think of single quotes as a mean of escaping things.
Well… escaping _more_ things than double quotes do.
====


=== Check mandatory parameters

In functions or for the script itself, make sure you check early that parameters that ought to be set (and, if relevant, non-empty) are indeed set.

[source, bash]
----
file=${1:?No file given.}
message=${2?No message given.}

printf '%s\n' "$message" > "$file"
----

* [x] `:?` checks that the variable is set and non-empty, and exits if it is unhappy.
* [x] `?` is similar, but accepts empty values.
* [x] A custom message can be written after the `?`, but is optional.

.Idem for functions
[source, bash]
----
my_function() {
    local file=${1:?No file given.}
    local message=${2?No message given.}

    printf '%s\n' "$message" > "$file"
}

my_function "$@"
----

Just like `set -e`, this can avoid “snowballing” effects and catch issues early.


=== Storing paths

Do _not_ include trailing slashes when storing directory paths in variables.

[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
p=foo/bar/
p+=plop/

readonly MY_CONST=/home/jenkins/
----
|
[source, bash]
----
p=foo/bar
p+=/plop

readonly MY_CONST=/home/jenkins
----
|===

This will cause fewer surprises, especially considering that most native tools (`find`, `pwd`, `dirname`, `basename`, etc.) output paths like this.

When _executing commands_, however, you _can_ add a slash to show that an argument is a directory path:

`mkdir -p foo/bar/plop/` +
`+rm -vfr -- "${my_temp_dir:?}"/+`


== Tips

[[basedir]]
=== Script location and project root

A very common need in scripts is to know where the script stands – and by extension, getting the path to the root of the project.
(Also see <<wd>>.)

.Getting a clean absolute path to the directory holding the script
====
[source, bash]
----
BASEDIR=$(cd "$(dirname "$0")" && pwd -P)
readonly BASEDIR
----

A version relying on `readlink` is also popular, but it is less portable.
====

In order to get the project’s root, if the script is in a subdirectory (typically `scripts/`), use the same approach but append one `/..` *for each subdirectory level*:

.Getting a clean absolute path to the project’s root when the script is one level below
====
[source, bash]
----
ROOTDIR=$(cd "$(dirname "$0")"/.. && pwd -P)
readonly ROOTDIR
----

You may be tempted to use `ROOTDIR=${BASEDIR}/..` but it would be less readable in debug logs, and harder to manipulate later.
The `pwd`-based approach gets rid of `..`-s.
====


=== Boolean impersonations

There are no real booleans in Bash.
We mostly have to work with:

* strings that are empty or not;
* commands (such as `true` and `false`) that exit with a status of success or failure;
* dumb string comparisons.

An often used approach is to put literally the strings “true” or “false” in variables.
However, *this has downsides*, and the way to use such variables is *not always clear*:

[source, bash]
----
OVERKILL_CONST_TRUE=true
OVERKILL_CONST_FALSE=false

dirty_true=true
dirty_false=false

if [[ $dirty_false ]]
then
    echo 'Not empty, so the test succeeds!'
else
    echo 'Too bad, we won’t get there.'
fi

if [[ $dirty_false = true ]]
then
    echo WOULOULOU
    exit 1
else
    echo 'This is better, but error- and typo-prone.'
fi

if [[ $dirty_false = $OVERKILL_CONST_TRUE ]]
then
    echo WOULOULOU
    exit 1
else
    echo 'Kinda safer, but even with better names it is quite weird.'
fi

if "$dirty_false"
then
    echo WOULOULOU
    exit 1
else
    echo 'It technically works, but do we really want to execute'
    echo 'random programs if a bug puts garbage in our variable?'
fi
----

This yields:

[source]
----
Not empty, so the test succeeds!
This is better, but error- and typo-prone.
Kinda safer, but even with better names it is quite weird.
It technically works, but do we really want to execute
random programs if a bug puts nonsense in our variable?
----

It is often *simpler and therefore recommended to use the “empty or not” paradigm*, conjointly with the default operator of `{startsb} / {startsb}{startsb} / test` (negated with `-z` or `!`):

[source, bash]
----
this_one_is_true=1
this_one_is_true_as_well=PLOP

this_one_is_false=''
unset -v this_one_is_false_as_well

if [[ $this_one_is_true && $this_one_is_true_as_well &&
    ! $this_one_is_false && -z $this_one_is_false_as_well ]]
then
    echo OK
else
    echo Bug
fi
----

[NOTE]
====
It is also technically possible to define (and re-define if needed) functions instead of basic variables, and then ditch `{startsb} / {startsb}{startsb} / test` entirely, but that may look weird to some:

[source, bash]
----
this_one_is_true() { true; }
this_one_is_true_as_well() { return 0; }

this_one_is_false() { false; }
this_one_is_false_as_well() { return 1; }

if this_one_is_true && this_one_is_true_as_well &&
    ! this_one_is_false && ! this_one_is_false_as_well
then
    echo OK
else
    echo Bug
fi
----
====


=== Sets

Sets in programming are basically bags which contain some objects (and, conversely, that do not contain any other object), with generally no notion of order whatsoever.

A nice way to obtain this in Bash is to use an associative array with values that do not really matter.
The keys tell you which items are in the set.

[source, bash]
----
declare -A my_set

for a in foo bar
do
    for b in plop yo
    do
        my_set[${a}_${b}]=1
    done
done

if [[ ${my_set['foo_plop']} ]]
then
    echo foo_plop in it
fi

if [[ ! ${my_set['foo_bar']} ]]
then
    echo foo_bar NOT in it
fi

printf 'All items:'
# “!” before name to get keys.
printf ' %q' "${!my_set[@]}"
echo
----

This yields:

[source]
----
foo_plop in it
foo_bar NOT in it
All items: foo_yo bar_yo foo_plop bar_plop
----

[NOTE]
====
Generally speaking, if you’re considering to type “find item in array in bash” in a search engine, it means that you should have used an associative array instead of (or in addition to) an indexed array.
====


=== Passing arrays to functions

==== Break it down into pieces

The naive approach, which is mostly for indexed arrays, is to individually pass all the items to the function:

[source, bash]
----
foo() {
    printf '[%s]' "$@"
    echo
}

bar() {
    local t=(first "$@")
    t+=(last)
    foo "${t[@]}"
}

initial=(
    yo
    yeah
)

bar "${initial[@]}"
----

This yields:

[source]
----
[first][yo][yeah][last]
----

This is quite convenient for simple cases, but things get hairy quite quickly if you need multiple arrays or if the function also have other parameters.
You may then need to:

* take the size of each array as extra parameters to know where they end;

* use notations like `"${@:2:2}"` to “skip `$1` and then take only two parameters from there”.

Furthermore, associative arrays are much harder to pass like this: you would need to provide the keys and values separately and perform arithmetical operations to know which function argument is the value for which other argument.

Fortunately, there are other ways to do that, as we’ll see in the following sections.


==== Quick and dirty: use the array as a global variable

(Basically, do not declare it with `local`).
At least document the function, though, to explain that it expects a variable named _XXX_ of type _YYY_ to be initialized and to contain stuff that means _ZZZ_:

[source, bash]
----
# Expects a global “global_indexed_array” indexed array to
# be defined and to contain at least two example words.
plop() {
    printf '%s\n' "${global_indexed_array[1]}"
}

declare -A global_associative_array=([foo]=bar)
declare -Ag more_explicitly_global_associative_array=([foo]=bar)

global_indexed_array=(foo bar)
declare -g more_explicitly_global_indexed_array=(foo bar)
declare -ag more_explicitly_global_more_explicitly_indexed_array=(foo bar)

plop
----

This yields `bar`.

[TIP]
====
When a global variable is especially important and gets manipulated by lots of functions, it can be worth marking it as special by prefixing its name with an underscore, for example: `_global_metadata`, `_config`, etc.
====


[[pass_by_name]]
==== Pass it by name

`local` (and `declare`) support a `-n` flag:

> make NAME a reference to the variable named by its value

[source, bash]
----
# $1    Name of string variable to print with the “A” prefix.
# $2    Name of indexed array to print with the “B” prefix.
# $3    Name of associative array to print with the “C” prefix.
foo() {
    local -n str=$1
    local -n index=$2
    local -n assoc=$3

    printf 'A %s=%q\n' "$1" "$str"

    printf 'B %s=[' "$2"
    printf ' %q' "${index[@]}"
    echo ' ]'

    printf 'C %s (Keys)=[' "$3"
    printf ' %q' "${!assoc[@]}"
    echo ' ]'

    printf 'C %s (Vals)=[' "$3"
    printf ' %q' "${assoc[@]}"
    echo ' ]'
}

# Declare stuff.
x=plop
y=(yo yeah)
declare -A z=([tulip]=potato [tart]=pie)

# Provide the NAMES to the function.
foo x y z
----

This yields:

[source]
----
A x=plop
B y=[ yo yeah ]
C z (Keys)=[ tart tulip ]
C z (Vals)=[ pie potato ]
----

Those references are not just to read: they *can be used to modify* the targeted variable from within the function, and *the changes will be kept*.

[CAUTION]
====
An error will occur if the variable used as a reference has the same name as the one it is referencing (`local -n foo=foo`).

To make this very unlikely, you can use one (or both) of the following strategies:

* [x] Add a condition to *skip the declaration* altogether if the names are equal:
+
[source, bash]
----
if [[ $1 != ref ]]
then
    local -n ref=$1
fi
----

* [x] Give an *ugly name* via a double-underscore prefix to your local variable:
+
[source, bash]
----
local -n __ref=$1
----
====


=== Saving variables for later

If you need to store the values of variables for later (possibly even for another execution of the script, or for another script altogether), you can use `declare -p`:

> display the attributes and value of each NAME

This does not sound like much, but…

[source, bash]
----
username='Foo Bar'
password='tom@to>te$t'

config=(
    -e 'yo yeah'
    --plop
    omg
)

declare -p username password config > save.sh
----

.Content of `save.sh` after execution
[source, bash]
----
declare -- username="Foo Bar"
declare -- password="tom@to>te\$t"
declare -a config=([0]="-e" [1]="yo yeah" [2]="--plop" [3]="omg")
----

This prints *perfectly valid and robust commands* that can be run to *declare clones* of the given variables, and this works for *any kind of variable*, even associative arrays.

[TIP]
====
*Functions* can also be processed this way if you add the `-f` flag.
====

Once your saved definition file is created, you can load the values by sourcing that file with `.`:

[source, bash]
----
printf 'Before: Username: %s\n' "${username:-Empty}"
printf 'Before: Password: %s\n' "${password:-Empty}"
printf 'Before: Config:   %s\n' "${config[*]:-Empty}"

. save.sh

printf 'After:  Username: %s\n' "${username:-Empty}"
printf 'After:  Password: %s\n' "${password:-Empty}"
printf 'After:  Config:   %s\n' "${config[*]:-Empty}"
----

This yields:

[source]
----
Before: Username: Empty
Before: Password: Empty
Before: Config:   Empty
After:  Username: Foo Bar
After:  Password: tom@to>te$t
After:  Config:   -e yo yeah --plop omg
----


=== `{startsb}` and `{startsb}{startsb}`

There are a couple of differences between those.
This has been extensively covered on the web: +
https://stackoverflow.com/questions/3427872/whats-the-difference-between-and-in-bash

Here are two things to take away, though:

* `{startsb}{startsb}` is a *keyword* of the Bash language.
This gives it more control over how its arguments are expanded.
As a consequence, `{quot}`-s are not needed within it unless you have hardcoded spaces, <<quotes_in_assignments, much like on the right-hand side of assignments>>.
However, you should generally quote the right-hand side of `=` and `!=` tests, otherwise https://github.com/koalaman/shellcheck/wiki/SC2053[it gets treated like a globbing pattern].
All in all:
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
[[ "${foo}/plop" = "$(bar)" ]]
----
|
[source, bash]
----
[[ ${foo}/plop = "$(bar)" ]]
----
|===
+
[cols = "2*a", options = header]
|===
^.^| {do_not}
^.^| {instead}

|
[source, bash]
----
[ ${foo}/plop = $(bar) ]
----
|
[source, bash]
----
[ "${foo}/plop" = "$(bar)" ]
----
|===

* The arithmetic operators of `{startsb}{startsb}`, namely:
+
--
** `-eq`
** `-ne`
** `-lt`
** `-le`
** `-gt`
** `-ge`
--
+
… are open to code injection:
+
[source, bash]
----
  $ x='a[$(date)]'; [[ $x -eq 1 ]]
bash: ven. 25 mars 2022 18:27:42 CET: syntax error: invalid arithmetic operator (error token is ". 25 mars 2022 18:27:42 CET")
----
+
Solutions include:

** Using `{startsb}` for those operators.
** Sanitizing data that may be provided by an external entity and that should be an integer:
*** `x=${x//[!0-9-]/}`
*** `x=$(tr -cd 0-9- <<< "$x")` (same result)
** Not using externally provided data at all.
** For basic comparisons, using the string-based operators instead:
*** `=`
*** `!=`
** Relying on a program like `bc` or even `python` for arithmetics.


=== Traps and cleanup

If you need temporary files during your script’s execution, set up an automatic cleanup for when the script exits:

[source, bash]
----
trap_exit() { <1>
    if [[ $TMP_DIR ]] <2>
    then
        rm -fr -- "$TMP_DIR" <3>
    fi
}

unset -v TMP_DIR <4>
trap trap_exit EXIT <5>

TMP_DIR=$(mktemp -d […]) <6>
----
<1> Cleanup function.

<2> Don’t attempt anything funny if the temporary-directory-related global variable is not set or if it is empty.

<3> Deletion.

<4> Make sure we don’t delete random stuff if there was an exported value and if we fail prematurely, before having created the “real” temporary directory.

<5> Set up the “trap” for cleanup on exit.
See `help trap` for more.

<6> Create the temporary directory however you like, and initialize the global variable.


== Unit tests

It is fairly easy to test functions without resorting to overkill third-party frameworks, and this can make development a lot easier.


=== Clean up your codebase

Let’s consider this script, standing alone at the root of a project:

.`main-script.sh`
[source, bash]
----
#! /usr/bin/env bash

# Read JSON on the standard input, and print it
# back on the standard output, after having set
# the value of the “foo” property to a new given
# value if and only if there was a “foo” property
# in the input to begin with.
#
# $1    New value for “foo”.
change_stuff_if_set() {
    local data
    # Grab and save standard input.
    data=$(cat)

    # See if there is a “foo” property.
    local foo
    if ! foo=$(jq '.foo // empty' <<< "$data")
    then
        # Give up if jq is not happy at all.
        echo 'Error: Could not parse JSON.'
        return 1
    fi

    if [[ $foo ]]
    then
        # Change the value of “foo”.
        data=$(jq --arg x "$1" '.foo = $x' <<< "$data")
    fi

    # Give back the (possibly modified) data.
    printf '%s\n' "$data"
}

change_stuff_if_set palala << '_JSON_'
{
    "foo": "bar",
    "plop": 12
}
_JSON_
----

To make testing easier, we’ll split that file in two:

. a functions-dedicated file;
. the main file, the one to be called by users.

.`main-script.sh`
[source, bash]
----
#! /usr/bin/env bash

# Take note of where this script stands.
BASEDIR=$(cd "$(dirname "$0")" && pwd -P)
readonly BASEDIR

# Load (“source”) the functions.
. "$BASEDIR"/functions.sh

change_stuff_if_set palala << '_JSON_'
{
    "foo": "bar",
    "plop": 12
}
_JSON_
----

.`functions.sh`
[source, bash]
----
# Read JSON on the standard input, and print it
# back on the standard output, after having set
# the value of the “foo” property to a new given
# value if and only if there was a “foo” property
# in the input to begin with.
#
# $1    New value for “foo”.
change_stuff_if_set() {
    local data
    # Grab and save standard input.
    data=$(cat)

    # See if there is a “foo” property.
    local foo
    if ! foo=$(jq '.foo // empty' <<< "$data")
    then
        # Give up if jq is not happy at all.
        echo 'Error: Could not parse JSON.'
        return 1
    fi

    if [[ $foo ]]
    then
        # Change the value of “foo”.
        data=$(jq --arg x "$1" '.foo = $x' <<< "$data")
    fi

    # Give back the (possibly modified) data.
    printf '%s\n' "$data"
}
----

[NOTE]
====
The functions-dedicated file does not necessarily need a shebang (unless linters complain that they have no idea which shell you’re using), as it is not meant to be *executed* but *sourced*.
====


=== Create your first test file

Add a `test/` directory to your project to keep things organized a little bit.
Then add a file like this to it:

.`test/change_stuff_if_set.sh`
[source, bash]
----
test "$(
    change_stuff_if_set yo << '_JSON_'
{
    "foo": "bar",
    "plop": 12
}
_JSON_
)" = '{
  "foo": "yo",
  "plop": 12
}'
----

On its own, it’s a bit hard to run properly:

* it needs `change_stuff_if_set` to be defined;

* it will not necessarily detect failing assertions.

This is where test runners come into play.


=== Create a test runner

This is basically a `run-tests.sh` script that will:

. fetch files from `test/`;
. load their content while making sure the required functions are defined;
. tell you if a commands (typically a `test` call) fails within one of the test files.

A minimalist approach would look like this:

.`run-tests.sh`
[source, bash]
----
#! /usr/bin/env bash

# Print what’s happening and stop if something fails.
set -ex

BASEDIR=$(cd "$(dirname "$0")" && pwd -P)
readonly BASEDIR

# Load this once and for all.
. "$BASEDIR"/functions.sh

# Find, print, loop through files.
# Could be done without the null-separated thingies
# (i.e., “find … -type f | while read -r path”)
# but it’s more robust this way.
find "$BASEDIR"/test/ -type f -print0 | while read -rd '' path
do
    # Load a test file.
    . "$path"
done

# If we reach this, it means that all tests passed.
echo 'OK!'
----

Then just run `./run-tests.sh` and you’ll get your results.

A more fancy test runner can be created like this:

.`run-tests.sh`
[source, bash]
----
#! /usr/bin/env bash

set -e

BASEDIR=$(cd "$(dirname "$0")" && pwd -P)
readonly BASEDIR
export BASEDIR

# Adapt if some tests files have overly long paths,
# but don’t increase that too much either.
declare -ri TEST_LOG_WIDTH=70

if [[ $1 = '-h' || $1 = '--help' ]]
then
    echo 'Usage: No arg to run all; give test paths (files or directories) to run just them.'
    exit 0
fi

# If needed, create a sandbox for tests that fiddle with files.
# Unset the global variable to avoid deleting unrelated stuff
# if the script somehow crashes between the trap setup
# and the temporary directory creation.
#clean_up() {
#    set +x
#
#    if [[ $TEST_TMP && -d $TEST_TMP ]]
#    then
#        rm -r -- "$TEST_TMP"
#    fi
#}
#unset -v TEST_TMP
#trap clean_up EXIT
#TEST_TMP=$(mktemp -d "${TMPDIR:-/tmp}"/bash-tests-XXXXXXXX)
#readonly TEST_TMP
#export TEST_TMP
# [End of sandbox-related code to uncomment]

# Keep track of which files caused issues.
unset -v failure_paths
failure_paths=()

# Count all test files.
declare -i nb=0
while read -rd '' path
do
    # Padded relative path to test file.
    printf "%-${TEST_LOG_WIDTH}s" "$(printf '%q ' "${path#"$(pwd)/"}")"
    # Only activate “-x” at the last moment,
    # and keep the output in your pocket for now.
    if output=$(
        bash -exc '
            # Automatically load functions for each test file.
            . "$BASEDIR"/functions.sh
            # Load the test file.
            . "$0"
        ' "$path" 2>&1
    )
    then
        echo '[OK]'
    else
        echo '[Failure]'
        # Display the whole “set -x”-generated blurb (as well as
        # whatever the test script may have printed) to help
        # fix the error.
        printf '%s\n' "$output"
        # Take a note of the file that caused the failure.
        failure_paths+=("$path")
    fi

    # […]
    # When using a temporary directory, you can empty it here
    # if you want it to be clean before each test file execution.

    ((nb++)) || true
done < <(
    # If specific test files were given as arguments,
    # run only those instead of fetching all test files.
    find "${@:-${BASEDIR}/test/}" -type f -print0
)

# Display a summary.
if [[ ${#failure_paths[@]} -eq 0 ]]
then
    printf 'All clear. (%d files)\n' "$nb"
    exit 0
else
    echo
    echo 'Failures:'
    echo
    printf '  - %q\n' "${failure_paths[@]#"$(pwd)/"}"
    echo
    exit 1
fi
----

The result looks like this:

image:run-tests.gif[Test run animation]


=== Improve your test files

Now that our test files are set, via `set / bash -e`, to halt upon error statuses, we can add lots of assertion-like instructions to them:

.`test/change_stuff_if_set.sh`
[source, bash]
----
# Check that the function returns with a success status:
change_stuff_if_set yo <<< '{}'

# Check output:
test "$(change_stuff_if_set yo <<< '{}')" = '{}'

# Check output for multiple input values (each with
# a corresponding “expected” value):
# (Using “>” as a custom word-splitting character.)
while IFS=$'>\n' read -r input expected
do
    # Use jq’s compacting feature to make
    # the JSON comparison formatting-agnostic.
    test $(
        # Actual:
        change_stuff_if_set yo <<< "$input" | jq -c
    ) = $(
        # Expected:
        jq -c <<< "$expected"
    )
done << '_INPUT_AND_EXPECTED_'
{}>{}
{ "plop": "yeah" }>{ "plop": "yeah" }
{ "foo": "yeah" }>{ "foo": "yo" }
_INPUT_AND_EXPECTED_

# Check that the function returns with a failure status:
! change_stuff_if_set yo <<< 'not-valid-JSON'

# Comment-like no-op that will be printed exactly once by “set -x”
# (while an “echo” would make the message appear twice,
# and a “normal” Bash comment would not appear at all).
: This explains the next assertion


for_demo_purposes() {
    exit 1
}

# Check that a function *exits* from its current shell
# (not just “returns”) with a failure status:
if (for_demo_purposes; true)
then
    exit 1
fi
----

[TIP]
====
If you do not feel confident enough with the big bad `set -e`, you can always do explicit things like:

[source, bash]
----
if [[ $actual != $expected ]]
then
    exit 1
fi
----
====


=== Mock annoying commands

==== Rationale

Sometimes during tests, it is not desirable to actually execute some commands, typically those that send HTTP requests:

.`functions.sh`
[source, bash]
----
# $1    Directory name.
get_file() {
    curl -L "https://yseop.com/${1:?}/file.json"
}
----

In such cases, it is possible to *define a function* with the same name as the command we want to avoid executing.
The function will take precedence and overshadow the command.

.`my-tests.sh`
[source, bash]
----
curl() {
    # No-op for now.
    :
}

# Check that the function does not crash.
if ! get_file foo
then
    echo Test failed
    exit 1
fi
----

Building on that, you can mock the *output* of the command:

[source, bash]
----
curl() {
    echo '{ "status": "ok" }'
}

if [[ $(get_file foo) != '{ "status": "ok" }' ]]
then
    echo Test failed
    exit 1
fi
----

[NOTE]
====
Sometimes, one function calls another Bash script, thus spawning a subprocess.
In such cases, you may need to export your mock via `export -f my_mock` in your test so that the final Bash script uses your mock instead of the real command.
====


==== Argument checks

It is generally a good idea to check that the mock was called with the *expected arguments*:

[source, bash]
----
curl() {
    if [[ $# -eq 2 && $1 = -L && $2 = https://yseop.com/foo/file.json ]]
    then
        echo '{ "status": "ok" }'
    else
        echo Test failed
        exit 1
    fi
}
----

If *multiple calls*, with different parameters or expected output, have to be made to the mock, you can:

. Define the function over and over again (cumbersome but straightforward):
+
[source, bash]
----
curl() {
    if [[ $# -eq 2 && $1 = -L && $2 = https://yseop.com/foo/file.json ]]
    then
        echo '{ "status": "ok" }'
    else
        echo Test failed
        exit 1
    fi
}

[[ $(get_file foo) = '{ "status": "ok" }' ]]

curl() {
    if [[ $# -eq 2 && $1 = -L && $2 = https://yseop.com/bar/file.json ]]
    then
        echo '{ "fruit": "apricot" }'
    else
        echo Test failed
        exit 1
    fi
}

[[ $(get_file bar) = '{ "fruit": "apricot" }' ]]
----

. Use a single definition with more possibilities:
+
[source, bash]
----
curl() {
    if [[ $# -eq 2 && $1 = -L ]]
    then
        case "$2" in
            https://yseop.com/foo/file.json)
                echo '{ "status": "ok" }'
                ;;

            https://yseop.com/bar/file.json)
                echo '{ "fruit": "apricot" }'
                ;;

            *)
                echo Test failed
                exit 1
                ;;
        esac
    else
        echo Test failed
        exit 1
    fi
}

[[ $(get_file foo) = '{ "status": "ok" }' ]]
[[ $(get_file bar) = '{ "fruit": "apricot" }' ]]
----

. Use global variables for the expected parameters and / or for the output:
+
[source, bash]
----
curl() {
    if [[ $# -eq 2 && $1 = -L && $2 = ${_expected_curl_arg_2?} ]]
    then
        printf '%s\n' "${_mocked_curl_output?}"
    else
        echo Test failed
        exit 1
    fi
}

_expected_curl_arg_2=https://yseop.com/foo/file.json
_mocked_curl_output='{ "status": "ok" }'
[[ $(get_file foo) = '{ "status": "ok" }' ]]

_expected_curl_arg_2=https://yseop.com/bar/file.json
_mocked_curl_output='{ "fruit": "apricot" }'
[[ $(get_file bar) = '{ "fruit": "apricot" }' ]]
----


==== Call counter

If *one execution* of the function to be tested generates *multiple distinct calls* to the mock, you can use a global counter to differentiate the calls (or just to check that the number of calls is the expected count).
The tricky part is that since most of the time commands and functions are called from within subshell environments, the counter’s increment is lost between calls if you just use a variable, and you have to write its value to a temporary file to make it _really_ shared:

[source, bash]
----
echo 0 > "$TEST_TMP"/calls

curl() {
    # Get
    local nb_calls=$(cat "$TEST_TMP"/calls)
    # Increment
    ((nb_calls++)) || true
    # Save
    printf '%d\n' "$nb_calls" > "$TEST_TMP"/calls

    if [[ $# -eq 2 && $1 = -L ]]
    then
        if [[ $nb_calls -eq 1 && $2 = https://yseop.com/foo/file.json ]]
        then
            echo '{ "status": "ok" }'
        elif [[ $nb_calls -eq 2 && $2 = https://yseop.com/bar/file.json ]]
        then
            echo '{ "fruit": "apricot" }'
        else
            echo Test failed
            exit 1
        fi
    else
        echo Test failed
        exit 1
    fi
}

[[ $(get_file foo) = '{ "status": "ok" }' ]]
[[ $(get_file bar) = '{ "fruit": "apricot" }' ]]
# Check total number of calls to the mock.
[[ $(cat "$TEST_TMP"/calls) -eq 2 ]]
----

[TIP]
====
This counter trick is also useful to make sure some operations are performed in the expected order.
====


==== Tips for high argument counts

If the number of “expected arguments” to check is too high, there are several ways to simplify the process (often at the cost of robustness, though):

.Dirty check with possible false positives if some arguments contain spaces
[source, bash]
----
curl() {
    if [[ $* = '-L https://yseop.com/foo/file.json' ]]
    then
        echo '{ "status": "ok" }'
    else
        echo Test failed
        exit 1
    fi
}
----

.Dirty check with possible false positives if some arguments contain a character that, at least, we can choose
[source, bash]
----
curl() {
    if (
        IFS='|'
        [[ $* = '-L|https://yseop.com/foo/file.json' ]]
    )
    then
        echo '{ "status": "ok" }'
    else
        echo Test failed
        exit 1
    fi
}
----

.Check with possible false positives if some arguments contain a newline; version with global expectation array variable
[source, bash]
----
curl() {
    if (
        IFS=$'\n'
        [[ $* = ${_expected[*]} ]]
    )
    then
        echo '{ "status": "ok" }'
    else
        echo Test failed
        exit 1
    fi
}

_expected=(
    -L
    https://yseop.com/foo/file.json
)
[[ $(get_file foo) = '{ "status": "ok" }' ]]
----


=== Running Bash tests via Jenkins

To automatically run the Bash unit tests during the project’s build, you can add a dedicated stage (possibly in parallel to some other tests):

.`Jenkinsfile`
[source, groovy]
----
pipeline {
    // […]
    stages {
        // […]
        stage('Script tests') {
            steps {
                sh('./path/to/run-tests.sh')
            }
        }
        // […]
    }
}
----

[CAUTION]
====
Make sure `run-tests.sh` is executable (`chmod u+x`).
====


== Useful functions and snippets

=== Checking for the presence of some executables

[source, bash]
----
# $@  Name of executables that must be found.
#
# Exit brutally with ≠0 status if at least one is missing.
check_progs() {
    local error
    local p

    for p
    do
        if ! type "$p" &> /dev/null
        then
            printf 'The command or program “%s” does not seem to be available.\n' "$p" >&2
            error=1
        fi
    done

    if [[ $error ]]
    then
        exit 1
    else
        return 0
    fi
}
----

.Usage
====
[source, bash]
----
check_progs curl jq unzip yq column
----
====


=== JSON and jq

==== Array length

[source, bash]
----
# Wrapper around jq’s “length”, geared towards arrays.
#
# <Empty input>   ⇒ 0
# null            ⇒ 0
# []              ⇒ 0
# ["foo", "bar"]  ⇒ 2
# Non-array input such as “{"foo":12}” or “"string"”
#                 ⇒ 0
#
# Works either with $1 or with stdin.
# $1 takes precedence, even if empty, as long as explicitly defined.
#
# Note that:
# * “safe” is relative: we assume the JSON is correct
#   (just perhaps empty or non-array).
# * jq will run its script for each input, so “[][]” ⇒ 0<NEWLINE>0.
safe_jq_length() {
    local data
    data=${1-$(cat /dev/stdin)}
    jq '(arrays | length) // 0' <<< "${data:-[]}"
}
----

.Usage
====
.With argument
[source, bash]
----
if (($(safe_jq_length "$json") > 10))
then
    echo 'Big array'
fi
----

.With stdin
[source, bash]
----
length=$(command-producing-json | safe_jq_length)
----
====


==== Array non-emptiness

(Requires the previous snippet.)

[source, bash]
----
# Wrapper around “safe_jq_length” to see if that length is at least one.
# $1 or stdin supported.
# While “safe_jq_length” accepts multiple inputs due to how jq works,
# this test function only considers the first one.
# Note that negating the result of this function does not result
# in a “is empty JSON array” test, but rather in a “is either an
# empty JSON array or something that is not an array” test.
is_nonempty_json_array() {
    local data
    data=${1-$(cat /dev/stdin)}
    (($(safe_jq_length "$data" | head -1) > 0))
}
----

.Usage
====
.With argument
[source, bash]
----
if is_nonempty_json_array "$projects_json"
then
    display_projects "$projects_json"
fi
----

.With stdin
[source, bash]
----
if command-producing-json | is_nonempty_json_array
then
    echo 'It works'
fi
----
====


=== UUIDs

[source, bash]
----
# Rough check to see if a string looks like a UUID.
# Perhaps a bit too loose in some regards, but it makes sure
# we’ve got only hex digits and hyphens, with the correct amounts and order.
#
# Works either with $1 or with stdin.
# $1 takes precedence, even if empty, as long as explicitly defined.
#
# Returns with 0 status only if the input is UUID-like.
is_uuid() {
    local data
    data=${1-$(cat /dev/stdin)}
    grep -xEq '[[:xdigit:]]{8}(-[[:xdigit:]]{4}){3}-[[:xdigit:]]{12}' <<< "$data"
}
----

.Usage
====
.With argument
[source, bash]
----
if is_uuid "$data"
then
    do_stuff "$data"
fi
----

[source, bash]
.With stdin
----
if ! command | is_uuid
then
    echo 'Something went wrong.' >&2
    exit 1
fi
----
====


=== Version number comparisons

While basic cases can be handled with `sort -V` alone, sometimes we need to make sure that our own product’s versions are sorted in such a way that versions with suffixes (snapshots, alpha releases, milestones…) are treated as being lower than suffix-less versions.
The following functions help in that regard.

[source, bash]
----
# Filter (reads data on stdin) to sort version numbers.
# Basically uses “sort -V” but also considers suffix-less versions
# to be “better” (higher) than versions with suffixes.
# (By “suffix”, we mean “hyphen and then more text”.)
# No difference is made between the different kinds of suffixes, though,
# so the algorithm falls back to lexicographical order then.
#
# 3.2.1 > 3.2.0 > 3.2.0-SNAPSHOT / …-M01 / …-alpha-foo
# 3.2.0-SNAPSHOT > 3.1.9
#
# If you’re getting your version numbers from jq, make sure you
# avoid nulls by using something like “.version // empty”.
#
# All versions from the input are printed back to stdout, with the highest
# version coming first (so a “head -1” can get the max and kill the processes
# to save time and resources).
sort_versions() {
    # Use a dummy suffix that is lexicographically very high, so that “sort -V”
    # considers “real”, suffix-less versions as “better” than snapshots and the like.
    # Remove the dummy suffix at the end, of course.
    # We ignore case to avoid getting alpha releases in weird places (may get
    # considered higher than no-suffix otherwise due to “a” coming before “Z”,
    # and this may even be locale-dependent).
    sed 's/^[^-]*$/&-ZZZZZ/' |
            sort --ignore-case -rV |
            sed 's/-ZZZZZ$//'
}

# Wrapper around “sort_versions” to get only the max.
get_highest_version() {
    sort_versions | head -1
}
----

.Usage
====
.Comparing the versions found in v1 v2 and v3 variables
[source, bash]
----
if [[ $(printf '%s\n' "$v1" "$v2" "$v3" | get_highest_version) = "$v1" ]]
then
    echo 'The first is the highest.'
fi
----
====


=== Regular readable file test

Nothing extremely complicated here but it’s annoying to type so we often end up with just one of the tests instead of three, and with a vague error message.

[source, bash]
----
# Utility to log stuff if a file does not exist
# or is not a regular file, etc.
#
# $1  Path to file (Mandatory, not empty)
#
# Returns with 0 status iff the path leads to
# a regular file readable by the user.
is_regular_readable_file() {
    local f=${1:?No path given.}
    if [[ ! -e $f ]]
    then
        printf 'File not found: %q\n' "$f" >&2
        return 1
    elif [[ ! -f $f ]]
    then
        printf 'Not a regular file: %q\n' "$f" >&2
        return 2
    elif [[ ! -r $f ]]
    then
        printf 'File not readable: %q\n' "$f" >&2
        return 3
    else
        return 0
    fi
}
----

.Usage
====
[source, bash]
----
is_regular_readable_file "$file" || exit
----
====


=== Curl with status, AKA “fail with body”

While using `curl` might seem simple, it’s often hard to get the correct behavior when juggling with options like `--fail`, `--silent` and `--show-error`: you can end up with…

* the shell script not caring about an HTTP error (due to `curl` exiting with a misleading status), or
* `curl` not printing anything in case of failures:
** not telling you what the body was,
** nor what HTTP status it got.

Consider using the following function to make logs more developer-friendly:

[source, bash]
----
# Adaptation of https://superuser.com/a/1641410
# Waiting for curl v7.76+ to have --fail-with-body…
#
# In this version, the body is printed to stdout if status OK,
# and stderr (only stderr!) if HTTP error.
# An attempt to pretty-format JSON errors will be made.
#
# $@    Extra args for curl.
curl_fail_with_body() {
    local OUTPUT_FILE
    OUTPUT_FILE=$(mktemp)

    local -i HTTP_CODE
    HTTP_CODE=$(curl --output "$OUTPUT_FILE" --write-out "%{http_code}" "$@")

    local body
    body=$(
        cat "$OUTPUT_FILE"
    )
    rm "$OUTPUT_FILE"

    if ((HTTP_CODE < 200 || HTTP_CODE > 299))
    then
        # If, e.g., “Could not resolve host”, we get 0 instead of a real HTTP status.
        if ((HTTP_CODE != 0))
        then
            # Printing to stderr dodges the $( … )-s, thus allowing
            # the user to actually see what went wrong.
            printf 'HTTP CODE = %s\n' "$HTTP_CODE" >&2
        fi
        if [[ $body ]]
        then
            # Print as human-friendly JSON if possible, otherwise raw.
            local body_for_humans
            if body_for_humans=$(jq -M . <<< "$body" 2> /dev/null)
            then
                printf '%s\n' "$body_for_humans" >&2
            else
                printf '%s\n' "$body" >&2
            fi
        fi
        return 22
    else
        # Normal execution, output to stdout.
        printf '%s\n' "$body"
        return 0
    fi
}
----

Depending on the context, you may want to add options directly in the `curl` call within that function, such as `--no-progress-meter` (for recent versions of `curl`) or `--location`.

.Usage
====
[source, bash]
----
unset -v args
args=(
    'https://acp.yseop-cloud.com/api/v1/platforms'
    --header 'Accept: application/json'
    --basic
    --user "${ACP_USR:?}:${ACP_PSW:?}"
    --no-progress-meter
)

curl_fail_with_body "${args[@]}"
----
====


=== Git Askpass

One way to provide credentials to Git, especially in CI/CD tools, is to set the `GIT_ASKPASS` environment variable to refer to a script providing a username and a password (generally a token) as requested.

Typically, in Jenkins:

[source, groovy]
----
environment {
    GIT_USER = 'jenkins-yseop'
    GIT_API_TOKEN = credentials('github')
    GIT_ASKPASS = "${env.WORKSPACE}/_git-askpass.sh"
}
----

Then make sure the mentioned file exists and is executable:

.`_git-askpass.sh`
[source, bash]
----
#! /usr/bin/env bash

# Meant to be used via the GIT_ASKPASS environment variable
# to provide Git credentials through environment variables.

case ${1,,} in
    username*)
        printf '%s\n' "$GIT_USER"
        ;;

    password*)
        printf '%s\n' "$GIT_API_TOKEN"
        ;;

    *)
        # Nothing to do.
        :
        ;;
esac
----

Subsequent Git commands should then “just work”.
Note, however, that cloning operations will have to be done through HTTPS, not via SSH, as this is not SSH authentication.
If you need your script to be both Jenkins-friendly and dev-friendly, you can add a layer of abstraction such as:

[source, bash]
----
# Get address for “git clone”.
# If GIT_ASKPASS is set, use HTTP. Otherwise use SSH.
#
# $1  Repository name. Example: “aa_afa-domains”
#
# stdout →
#     https://github.com/yseop/XXX.git
#     or
#     git@github.com:yseop/XXX.git
repository_name_to_cloning_address() {
    if [[ $GIT_ASKPASS ]]
    then
        # HTTP:
        printf 'https://github.com/yseop/%s.git\n' "${1:?Repository name}"
    else
        # SSH:
        printf 'git@github.com:yseop/%s.git\n' "${1:?Repository name}"
    fi
}
----


== Further reading and watching

* https://drive.google.com/file/d/1oSxE6qZXBAzRKEaVJnXdufy7jYvGaYf4/view?usp=sharing[Internal technical presentation (French).]

* `man bash` (`/` to search, `n` and `N` for next and previous occurrences).
You will be surprised by how much useful information is in there.
You can even run something like `man bash | grep -iC 5 'substring'` to quickly find stuff.

* https://google.github.io/styleguide/shellguide.html[The Google guide]. Their conventions are not 100% aligned with ours of course, but this contains useful tips, like the need to split `local foo=$(bar)` into two statements, etc.
